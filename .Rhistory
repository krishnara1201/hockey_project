# Extract date features
dtrain$year <- year(dtrain$date)
dtrain$month <- month(dtrain$date)
dtrain$day <- day(dtrain$date)
dtrain$day_of_week <- wday(dtrain$date, label = FALSE) # Sunday = 1, Saturday = 7
dtrain$is_weekend <- ifelse(dtrain$day_of_week %in% c(1, 7), 1, 0) # Weekend flag
# Cyclic encoding for month (to capture periodicity)
dtrain$month_sin <- sin(2 * pi * dtrain$month / 12)
dtrain$month_cos <- cos(2 * pi * dtrain$month / 12)
View(dtrain)
# Drop the original date column (not used directly in GBM)
dtrain$date <- NULL
load("final.RData")
# Sample dataset with a date column
# Convert the 'date' column to Date format
dtrain$date <- as.Date(dtrain$date)
# Extract date features
dtrain$year <- year(dtrain$date)
dtrain$month <- month(dtrain$date)
dtrain$day <- day(dtrain$date)
dtrain$day_of_week <- wday(dtrain$date, label = FALSE) # Sunday = 1, Saturday = 7
dtrain$is_weekend <- ifelse(dtrain$day_of_week %in% c(1, 7), 1, 0) # Weekend flag
# Cyclic encoding for month (to capture periodicity)
dtrain$month_sin <- sin(2 * pi * dtrain$month / 12)
dtrain$month_cos <- cos(2 * pi * dtrain$month / 12)
dtrain$week_sin <- sin(2 * pi * dtrain$week / 12)
# Drop the original date column (not used directly in GBM)
dtrain$date <- NULL
dtrain$month <- NULL
# Split the dtrainset into features and target
X <- dtrain[, !(names(dtrain) %in% c("salary"))]
y <- dtrain$salary
model.boost <- gbm(salary ~. ,
data = dtrain,
shrinkage = 1,
n.trees = 100)
str(dtrain)
# Convert all character columns to factors
data[] <- lapply(data, function(x) if (is.character(x)) as.factor(x) else x)
# Convert all character columns to factors
dtrain[] <- lapply(data, function(x) if (is.character(x)) as.factor(x) else x)
# Convert all character columns to factors
dtrain <- lapply(dtrain, function(x) if (is.character(x)) as.factor(x) else x)
View(dtrain)
dtrain
str(dtrain)
load("final.RData")
formula = "salary~date + cap + gp + injure + toi+ pts+ goal + pos + nat + takeaway + giveaway + relCorsi + pim + hits + pm "
library(lubridate)
# Sample dataset with a date column
# Convert the 'date' column to Date format
dtrain$date <- as.Date(dtrain$date)
# Extract date features
dtrain$year <- year(dtrain$date)
dtrain$month <- month(dtrain$date)
dtrain$day <- day(dtrain$date)
dtrain$day_of_week <- wday(dtrain$date, label = FALSE) # Sunday = 1, Saturday = 7
dtrain$is_weekend <- ifelse(dtrain$day_of_week %in% c(1, 7), 1, 0) # Weekend flag
# Cyclic encoding for month (to capture periodicity)
dtrain$month_sin <- sin(2 * pi * dtrain$month / 12)
dtrain$month_cos <- cos(2 * pi * dtrain$month / 12)
# Drop the original date column (not used directly in GBM)
dtrain$date <- NULL
dtrain$month <- NULL
# Convert all character columns to factors
dtrain <- apply(dtrain, MARGIN = 2, function(x) if (is.character(x)) as.factor(x) else x)
View(dtrain)
View(dtrain)
View(dtrain)
str(dtrain)
library(tidyverse)
load("final.RData")
formula = "salary~date + cap + gp + injure + toi+ pts+ goal + pos + nat + takeaway + giveaway + relCorsi + pim + hits + pm "
# Sample dataset with a date column
# Convert the 'date' column to Date format
dtrain$date <- as.Date(dtrain$date)
# Extract date features
dtrain$year <- year(dtrain$date)
dtrain$month <- month(dtrain$date)
dtrain$day <- day(dtrain$date)
dtrain$day_of_week <- wday(dtrain$date, label = FALSE) # Sunday = 1, Saturday = 7
dtrain$is_weekend <- ifelse(dtrain$day_of_week %in% c(1, 7), 1, 0) # Weekend flag
# Cyclic encoding for month (to capture periodicity)
dtrain$month_sin <- sin(2 * pi * dtrain$month / 12)
dtrain$month_cos <- cos(2 * pi * dtrain$month / 12)
# Drop the original date column (not used directly in GBM)
dtrain$date <- NULL
dtrain$month <- NULL
# Convert all character columns to factors
dtrain <- dtrain %>%
mutate(across(where(is.character), as.factor))
View(dtrain)
str(dtrain)
model.boost <- gbm(salary ~. ,
data = dtrain,
shrinkage = 1,
n.trees = 100)
View(model.boost)
summary(model.boost)
?gbm
# Set up
M <- 500
k <- 5
set.seed(31853071)
# Shrinkage parameter values
alphas <- c(0.005, 0.01, 0.02, 0.03, 0.04,
0.05, 0.06, 0.07, 0.08, 0.09,
0.10, 0.20, 0.30, 0.40, 0.50,
0.60, 0.70, 0.80, 0.90, 1)
n_alphas <- length(alphas)
cverror <- numeric(length = n_alphas)
Mvals <- numeric(length = n_alphas)
fit <- list(length = n_alphas)
for (i in 1:n_alphas) {
fit[[i]] <- fb.boost.shrink <- gbm(salary ~.,
data=dtrain,
distribution = "gaussian",
shrinkage = alphas[i],
n.trees = M,
bag.fraction = 1,
cv.folds = k
)
cverror[i] <- min(fit[[i]]$cv.error)
Mvals[i] <- which.min(fit[[i]]$cv.error)
}
plot(alphas, cverror, type = "b",
col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
main = "cross-validated error", xlab = "shrinkage", ylab="cv.error")
i <- which.min(cverror)
alpha <- alphas[i]
summary(fit[[i]])
# Shrinkage parameter values
Ms <- c(50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000)
n_Ms <- length(Ms)
cverror <- numeric(length = n_Ms)
Mvals <- numeric(length = n_Ms)
fit <- list(length = n_Ms)
for (i in 1:n_Ms) {
fit[[i]] <- fb.boost.shrink <- gbm(as.formula(formula2),
data=facebook3,
distribution = "gaussian",
shrinkage = alpha,
n.trees = Ms[i],
bag.fraction = 1,
cv.folds = k
)
cverror[i] <- min(fit[[i]]$cv.error)
}
for (i in 1:n_Ms) {
fit[[i]] <- model.boost.trees <- gbm(salary ~.,
data=dtrain,
distribution = "gaussian",
shrinkage = alpha,
n.trees = Ms[i],
bag.fraction = 1,
cv.folds = k
)
cverror[i] <- min(fit[[i]]$cv.error)
}
plot(Ms, cverror, type = "b",
col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
main = "cross-validated error", xlab = "Number of trees", ylab="cv.error")
i <- which.min(cverror)
i <- which.min(cverror)
M <- Ms[i]
summary(fit[[i]])
# Bag fractions parameter values
bag.fracs <- c(0.005, 0.01, 0.02, 0.03, 0.04,
0.05, 0.06, 0.07, 0.08, 0.09,
0.10, 0.20, 0.30, 0.40, 0.50,
0.60, 0.70, 0.80, 0.90, 1)
# Bag fractions parameter values
bag.fracs <- c(0.005, 0.01, 0.02, 0.03, 0.04,
0.05, 0.06, 0.07, 0.08, 0.09,
0.10, 0.20, 0.30, 0.40, 0.50,
0.60, 0.70, 0.80, 0.90, 1)
n_bag.fracs <- length(bag.fracs)
cverror <- numeric(length = n_bag.fracs)
Mvals <- numeric(length = n_bag.fracs)
fit <- list(length = n_bag.fracs)
for (i in 1:n_bag.fracs) {
fit[[i]] <- model.boost.trees <- gbm(salary ~.,
data=dtrain,
distribution = "gaussian",
shrinkage = alpha,
n.trees = M,
bag.fraction = bag.fracs[i],
cv.folds = k
)
cverror[i] <- min(fit[[i]]$cv.error)
}
# Bag fractions parameter values
bag.fracs <- c(0.05, 0.10, 0.20, 0.30, 0.40, 0.50,
0.60, 0.70, 0.80, 0.90, 1)
n_bag.fracs <- length(bag.fracs)
cverror <- numeric(length = n_bag.fracs)
Mvals <- numeric(length = n_bag.fracs)
fit <- list(length = n_bag.fracs)
for (i in 1:n_bag.fracs) {
fit[[i]] <- model.boost.trees <- gbm(salary ~.,
data=dtrain,
distribution = "gaussian",
shrinkage = alpha,
n.trees = M,
bag.fraction = bag.fracs[i],
cv.folds = k
)
cverror[i] <- min(fit[[i]]$cv.error)
}
plot(bag.fracs, cverror, type = "b",
col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
main = "cross-validated error", xlab = "Number of trees", ylab="cv.error")
i <- which.min(cverror)
bag.fraction <- bag.fracs[i]
summary(fit[[i]])
gbm.final <- fit[[i]]
# Convert the 'date' column to Date format
dtest$date <- as.Date(dtest$date)
# Extract date features
dtest$year <- year(dtest$date)
dtest$month <- month(dtest$date)
dtest$day <- day(dtest$date)
dtest$day_of_week <- wday(dtest$date, label = FALSE) # Sunday = 1, Saturday = 7
dtest$is_weekend <- ifelse(dtest$day_of_week %in% c(1, 7), 1, 0) # Weekend flag
# Cyclic encoding for month (to capture periodicity)
dtest$month_sin <- sin(2 * pi * dtest$month / 12)
dtest$month_cos <- cos(2 * pi * dtest$month / 12)
# Drop the original date column (not used directly in GBM)
dtest$date <- NULL
dtest$month <- NULL
# Convert all character columns to factors
dtest <- dtest %>%
mutate(across(where(is.character), as.factor))
View(dtest)
preds <- predict(gbm.final, dtest)
dtest$preds <- predict(gbm.final, dtest)
submission <- select(dtest, c(id, preds))
submission <- select(dtest, c(Id, preds))
View(submission)
write.csv(submission, "Sub.csv")
write.csv(submission, "Sub.csv",row.names = FALSE)
write.csv(submission, "Sub.csv",row.names = FALSE)
dtest$salary <- predict(gbm.final, dtest)
submission <- select(dtest, c(Id, salary))
write.csv(submission, "sub.csv",row.names = FALSE)
write.csv(submission, "sub.csv",row.names = FALSE)
write.csv(submission, "sub.csv",row.names = FALSE)
test = read.csv("sub.csv")
View(test)
View(dtest)
View(dtrain)
# Data exploration
cor_matrix <- cor(dtest[, c("salary", "gp", "pts", "toi", "relCorsi")])
corrplot::corrplot(cor_matrix)  # [3][7]
install.packages("corrplot")
corrplot::corrplot(cor_matrix)  # [3][7]
cor_matrix
# Data exploration
cor_matrix <- cor(dtrain[, c("salary", "gp", "pts", "toi", "relCorsi")])
corrplot::corrplot(cor_matrix)  # [3][7]
pairs(dtrain[, c("salary", "gp", "pts", "toi", "relCorsi")])
# Data exploration
cor_matrix <- cor(dtrain[, c("salary", "gp", "pts", "toi", "relCorsi",
"cap", "injure", "age", "takeaway", "giveaway")])
pairs(dtrain[, c("salary", "gp", "pts", "toi", "relCorsi",
"cap", "injure", "age", "takeaway", "giveaway")])
dtrain$lgp <- log(dtrain$gp)
plot(dtrain$gp, dtrain$salary)
plot(dtrain$;gp, dtrain$salary)
plot(dtrain$lgp, dtrain$salary)
plot(dtrain$gp, dtrain$salary)
pairs(dtrain[, c("salary", "gp", "pts", "toi", "relCorsi",
"cap", "injure", "age", "takeaway", "giveaway")])
install.packages("randomForest")
## Random Forest
library(randomForest)
model.rf <- randomForest(salary ~ .,
data = dtrain,
# Number of variates to select at each step
mtry = 3)
trainx <- dtrain[, get.explanatory_varnames(model.rf)]
## Random Forest
library(randomForest)
?get.explanatory_variates
??get.explanatory_variates
trainx <- select(dtrain, -c("Salary"))
# Need separate response and explanatory variate data frames
trainy <- dtrain[, "Salary"]
View(dtrain)
# Need separate response and explanatory variate data frames
trainy <- dtrain[, "salary"]
trainx <- select(dtrain, -c("salary"))
# Five fold cross-validataion
model.rfcv <- rfcv(trainx = trainx, trainy = trainy, cv.fold = 5)
# We can plot the results
with(model.rfcv, plot(n.var, error.cv, pch = 19, type="b", col="blue"))
model.rfcv$error.cv
importance(model.rf, type = 1)
model.rf <- randomForest(salary ~ .,
data = dtrain,
# Number of variates to select at each step
mtry = 3,
importance = TRUE)
importance(model.rf, type = 1)
varImpPlot(model.rf)
# Five fold cross-validataion
model.rfcv <- rfcv(trainx = trainx, trainy = trainy, cv.fold = 10)
# We can plot the results
with(model.rfcv, plot(n.var, error.cv, pch = 19, type="b", col="blue"))
importance(model.rf, type = 2)
varImpPlot(model.rf)
model.rf.2 <- randomForest(salary ~ pts + toi + takeaway,
data = dtrain,
importance = TRUE)
# Need separate response and explanatory variate data frames
trainy <- dtrain[, "salary"]
trainx <- select(dtrain, c("pts", "toi", "takeaway"))
# 10 fold cross-validataion
model.rfcv2 <- rfcv(trainx = trainx, trainy = trainy, cv.fold = 10)
# We can plot the results
with(model.rfcv2, plot(n.var, error.cv, pch = 19, type="b", col="blue"))
predict(model.rf.2, dtest)
submission.rf <- predict(model.rf.2, dtest)
write.csv2(submission.rf, "sub_rf.csv", row.names = FALSE)
submission.rf
dtest$salary <- predict(model.rf.2, dtest)
submission <- select(dtest, c(Id, salary))
write.csv(submission, "sub.csv",row.names = FALSE)
write.csv(submission, "sub_rf.csv",row.names = FALSE)
model.boost.trees <- gbm(salary ~ pts + toi + takeaway,
data=dtrain,
distribution = "gaussian",
shrinkage = alpha,
n.trees = M,
bag.fraction = bag.fraction,
cv.folds = k
)
model.gbm <- gbm(salary ~ pts + toi + takeaway,
data=dtrain,
distribution = "gaussian",
shrinkage = alpha,
n.trees = M,
bag.fraction = bag.fraction,
cv.folds = k
)
submission <- select(dtest, c(Id, salary))
write.csv(submission, "sub_gbm.csv",row.names = FALSE)
dtest$salary <- predict(model.gbm, dtest)
submission <- select(dtest, c(Id, salary))
write.csv(submission, "sub_gbm.csv",row.names = FALSE)
summary(model.gbm)
summary(model.boost)
summary(gbm.final)
summary(fit[[i]])
feature_sets <- list(
top3 = c("toi", "pts", "team"),
top5 = c("toi", "pts", "team", "nat", "takeaway"),
top7 = c("toi", "pts", "team", "nat", "takeaway", "pim", "giveaway"),
top9 = c("toi", "pts", "team", "nat", "takeaway", "pim", "giveaway", "hits", "age")
all = names(train_data)[names(train_data) != "salary"]
feature_sets <- list(
top3 = c("toi", "pts", "team"),
top5 = c("toi", "pts", "team", "nat", "takeaway"),
top7 = c("toi", "pts", "team", "nat", "takeaway", "pim", "giveaway"),
top9 = c("toi", "pts", "team", "nat", "takeaway", "pim", "giveaway", "hits", "age"),
all = names(train_data)[names(train_data) != "salary"]
)
feature_sets <- list(
top3 = c("toi", "pts", "team"),
top5 = c("toi", "pts", "team", "nat", "takeaway"),
top7 = c("toi", "pts", "team", "nat", "takeaway", "pim", "giveaway"),
top9 = c("toi", "pts", "team", "nat", "takeaway", "pim", "giveaway", "hits", "age"),
all = names(dtrain)[names(dtrain) != "salary"]
)
feature_sets[[1]]
n_features <- length(feature_sets)
feature_sets <- list(
top3 = c("toi", "pts", "team"),
top5 = c("toi", "pts", "team", "nat", "takeaway"),
top7 = c("toi", "pts", "team", "nat", "takeaway", "pim", "giveaway"),
top9 = c("toi", "pts", "team", "nat", "takeaway", "pim", "giveaway", "hits", "age"),
all = names(dtrain)[names(dtrain) != "salary"]
)
n_features <- length(feature_sets)
cverror <- numeric(length = n_features)
fit <- list(length = n_features)
for (i in 1:n_features) {
features <- feature_sets[[i]]
formula <- as.formula(paste("salary ~", paste(features, collapse = " + ")))
fit[[i]] <- model.boost.trees <- gbm(formula,
data=dtrain,
distribution = "gaussian",
shrinkage = alpha,
n.trees = M,
bag.fraction = bag.fraction,
cv.folds = k
)
cverror[i] <- min(fit[[i]]$cv.error)
}
plot(c(3,5,7,9,23), cverror, type = "b",
col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
main = "cross-validated error", xlab = "Number of trees", ylab="cv.error")
i <- which.min(cverror)
gbm.final <- fit[[2]]
dtest$salary <- predict(gbm.final, dtest)
submission <- select(dtest, c(Id, salary))
write.csv(submission, "sub.csv",row.names = FALSE)
source("C:/Users/Shreyash/OneDrive - University of Waterloo/Term 4B/STAT 444/Final Project/Model Building.R")
summary(gbm.final)
summary(model.gbm)
# Boosted trees
boostTree <- function(formula, data,
lam=0.01, M = 10,
control=rpart.control(), ...) {
# Break the formula into pieces
formula.sides <- strsplit(formula, "~")[[1]]
response.string <- formula.sides[1]
rhs.formula <- formula.sides[2]
# Construct the boost formula
bformula <- paste("resid", rhs.formula, sep=" ~ ")
# Initialize the resid and explanatory variates
resid <- get.response(formula, data)
xvars <- get.newdata(formula, data)
# Calculate the boostings
Trees <- Map(
function(i) {
# update data frame with current resid
rdata <- data.frame(resid=resid, xvars)
# Fit the tree
tree <- rpart(bformula, data = rdata, control=control, ...)
# Update the residuals
# (Note the <<- assignment to escape this closure)
resid <<- resid - lam * predict(tree)
# Return the tree
tree }
, 1:M)
# Return the boosted function
function(newdata){
if (missing(newdata)) {
predictions <- Map(function(tree) {
# Boost piece
lam * predict(tree)
}, Trees)
} else {
predictions <- Map(function(tree){
# New data needs to be a list
if (is.data.frame(newdata)) {
newdata.tree <- get.newdata(tree, newdata)
} else {
newdata.tree <- newdata
}
# Boost piece
lam * predict(tree, newdata=newdata.tree)
}, Trees)
}
# Gather the results together
Reduce(`+`, predictions)
}
}
boosted_model <- boostTree(salary~., data = dtrain, M = 100, lam = 0.01)
boosted_model <- boostTree(salary ~ ., data = dtrain, M = 100, lam = 0.01)
boosted_model <- boostTree("salary ~ .", data = dtrain, M = 100, lam = 0.01)
boosted_model <- boostTree("salary ~.", data = dtrain, M = 100, lam = 0.01)
# Helper
# It will also be handy to have a function that will return the
# newdata as a list, since that is what is expected by predict
# for that argument.
# We will write a function that takes a fitted tree (or any other fit)
# THis involves a little formula manipulation ... of interest only ...
get.newdata <- function(fittedTree, test.data){
f <- formula(fittedTree)
as.list(test.data[,attr(terms(f), "term.labels")])
}
#
# And a similar function that will extract the response values
# This is kind of hairy, formula manipulation ... feel free to ignore ...
get.response <- function(fittedTree, test.data){
f <- formula(fittedTree)
terms <- terms(f)
response.id <- attr(terms, "response")
response <- as.list(attr(terms, "variables"))[[response.id + 1]]
with(test.data, eval(response))
}
get.explanatory_varnames <- function(formula){
f <- as.formula(formula)
terms <- terms(f)
attr(terms, "term.labels")
}
# The remaining functions are the important ones
#
getTrees <- function(data, formula, B=100, ...) {
N <- nrow(data)
Trees <- Map(function(i){
getTree(formula,
getSample(data, N),
...)
},
1:B
)
Trees
}
boosted_model <- boostTree("salary ~.", data = dtrain, M = 100, lam = 0.01)
boosted_model <- boostTree("salary ~pts + toi + takeaway", data = dtrain, M = 100, lam = 0.01)
install.packages("rpart")
library(rpart)
boosted_model <- boostTree("salary ~pts + toi + takeaway", data = dtrain, M = 100, lam = 0.01)
pred <- boosted_model(dtest)
boosted_model <- boostTree("salary ~pts + toi + takeaway",
data = dtrain, M = 1000, lam = 0.01)
dtest$salary <- boosted_model(dtest)
submission <- select(dtest, c(Id, salary))
write.csv(submission, "sub_boosted.csv",row.names = FALSE)
